{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a267331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7064a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2819d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc71501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dbb04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 10:12:48 WARN Utils: Your hostname, ashwins-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.199 instead (on interface en0)\n",
      "22/11/01 10:12:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 10:12:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder\\\n",
    "                  .master(\"local\")\\\n",
    "                  .appName('new')\\\n",
    "                  .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c45275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. reading the data - Reader API\n",
    "#2. crunching of data - transformations\n",
    "#3. write the data back - Writer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8701edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\",True)\\\n",
    "         .option(\"inferSchema\",True)\\\n",
    "         .option(\"path\",\"orders-201019-002101.csv\")\\\n",
    "         .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d540a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"path\",\"/Users/ashwinpandey/Desktop/folder\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d238fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions are  1\n"
     ]
    }
   ],
   "source": [
    "print(\"number of partitions are \", orderDf.rdd.getNumPartitions())\n",
    "ordersRep = orderDf.repartition(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3807be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersRep.write.format(\"csv\")\\\n",
    "         .mode(\"overwrite\")\\\n",
    "         .option(\"path\",\"/Users/ashwinpandey/Desktop/folder\")\\\n",
    "         .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c233a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========\n",
    "#saving option\n",
    "# 1- overwrite\n",
    "# 2- append\n",
    "# 3- errorIfExists\n",
    "# 4- ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dce40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====\n",
    "#Parquet is the default file format in apache spark when we talk about structured api's\n",
    "#=====Spark File LayoutNumber of files is equal to number of partitions.\n",
    "# 1. simple repartition - repartition\n",
    "# 2. partitioning - partitionBy (allows partitioning pruning)\n",
    "# 3. bucketBy\n",
    "# 4. maxRecordsPerFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f68bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orderDf = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",True)\\\n",
    "        .option(\"inferSchema\",True)\\\n",
    "        .option(\"path\",\"orders-201019-002101.csv\")\\\n",
    "        .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a5c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\").partitionBy(\"order_status\")\\\n",
    "       .mode(\"overwrite\")\\\n",
    "       .option(\"path\",\"/Users/ashwinpandey/Desktop/folder\")\\\n",
    "       .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ae53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\",True)\\\n",
    "         .option(\"inferSchema\",True)\\\n",
    "         .option(\"path\",\"/Users/trendytech/Desktop/data/orders.csv\")\\\n",
    "         .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"avro\")\\\n",
    ".mode(\"overwrite\")\\.option(\"path\",\"/Users/trendytech/Desktop/newfolder4\")\\.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e55e89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e90ae372",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\",True)\\\n",
    "         .option(\"inferSchema\",True)\\\n",
    "         .option(\"path\",\"orders-201019-002101.csv\")\\\n",
    "         .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "555d68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c2affba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|   order_status|total_orders|\n",
      "+---------------+------------+\n",
      "|PENDING_PAYMENT|       15030|\n",
      "|       COMPLETE|       22900|\n",
      "|        ON_HOLD|        3798|\n",
      "| PAYMENT_REVIEW|         729|\n",
      "|     PROCESSING|        8275|\n",
      "|         CLOSED|        7556|\n",
      "|SUSPECTED_FRAUD|        1558|\n",
      "|        PENDING|        7610|\n",
      "|       CANCELED|        1428|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDf = spark.sql(\"select order_status, count(*) as total_orders from orders group by order_status\")\n",
    "resultDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56481ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|order_customer_id|total_orders|\n",
      "+-----------------+------------+\n",
      "|             1833|           6|\n",
      "|             1687|           5|\n",
      "|             1363|           5|\n",
      "|             5493|           5|\n",
      "|             2236|           4|\n",
      "|            10018|           4|\n",
      "|             3631|           4|\n",
      "|            12431|           4|\n",
      "|             7879|           4|\n",
      "|             5319|           4|\n",
      "|             2774|           4|\n",
      "|            10263|           4|\n",
      "|             4997|           4|\n",
      "|             1443|           4|\n",
      "|              569|           4|\n",
      "|             9213|           4|\n",
      "|             4573|           4|\n",
      "|             4588|           4|\n",
      "|            10111|           4|\n",
      "|             7948|           4|\n",
      "+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDf1 = spark.sql(\"select order_customer_id, count(*) as total_orders from orders where order_status = 'CLOSED' group by order_customer_id order by total_orders desc\")\n",
    "resultDf1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7e91b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table has 2 parts\n",
    "#1. data - warehouse - spark.sql.warehouse.dir\n",
    "#2. metadata - catalog metastore - memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6669a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\")\\\n",
    "       .mode(\"overwrite\")\\\n",
    "       .saveAsTable(\"orders1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe79a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d3b63e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderDf = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\",True)\\\n",
    "         .option(\"inferSchema\",True)\\\n",
    "         .option(\"path\",\"orders-201019-002101.csv\")\\\n",
    "         .load()\n",
    "spark.sql(\"create database if not exists retail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6acaa94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\")\\\n",
    "       .mode(\"overwrite\")\\\n",
    "       .saveAsTable(\"retail.orders2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e06b675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\")\\\n",
    "       .mode(\"overwrite\")\\\n",
    "       .saveAsTable(\"retail.orders3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "248e69d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "932296ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDf.write.format(\"csv\")\\\n",
    "       .bucketBy(4,\"order_customer_id\")\\\n",
    "       .sortBy(\"order_customer_id\")\\\n",
    "       .saveAsTable(\"retail.orders4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78b416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readind the un-structured data using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81616c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myregex = r'^(\\S+) (\\S+)\\t(\\S+)\\,(\\S+)'\n",
    "lines_df = spark.read.text(\"orders_new-201019-002101.csv\")\n",
    "#lines_df.printSchema()#lines_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8829ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+--------+----------+-----------+---------------+\n",
      "|order_id|      date|customer_id|         status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|      12111|       COMPLETE|\n",
      "|       4|2013-07-25|       8827|         CLOSED|\n",
      "|       5|2013-07-25|      11318|       COMPLETE|\n",
      "|       6|2013-07-25|       7130|       COMPLETE|\n",
      "|       7|2013-07-25|       4530|       COMPLETE|\n",
      "|       8|2013-07-25|       2911|     PROCESSING|\n",
      "|       9|2013-07-25|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25|       1837|         CLOSED|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df =lines_df.select(regexp_extract('value',myregex,1).alias(\"order_id\"),regexp_extract('value',myregex,2).alias(\"date\"),regexp_extract('value',myregex,3).alias(\"customer_id\"),regexp_extract('value',myregex,4).alias(\"status\"))\n",
    "final_df.printSchema()\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdc4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.select(\"order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5e23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         status|count|\n",
      "+---------------+-----+\n",
      "|PENDING_PAYMENT|    3|\n",
      "|       COMPLETE|    4|\n",
      "| PAYMENT_REVIEW|    1|\n",
      "|     PROCESSING|    1|\n",
      "|         CLOSED|    3|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.groupby(\"status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c8a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column String\n",
    "#Column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dca20cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|order_id|         order_date|\n",
      "+--------+-------------------+\n",
      "|       1|2013-07-25 00:00:00|\n",
      "|       2|2013-07-25 00:00:00|\n",
      "|       3|2013-07-25 00:00:00|\n",
      "|       4|2013-07-25 00:00:00|\n",
      "|       5|2013-07-25 00:00:00|\n",
      "|       6|2013-07-25 00:00:00|\n",
      "|       7|2013-07-25 00:00:00|\n",
      "|       8|2013-07-25 00:00:00|\n",
      "|       9|2013-07-25 00:00:00|\n",
      "|      10|2013-07-25 00:00:00|\n",
      "|      11|2013-07-25 00:00:00|\n",
      "|      12|2013-07-25 00:00:00|\n",
      "|      13|2013-07-25 00:00:00|\n",
      "|      14|2013-07-25 00:00:00|\n",
      "|      15|2013-07-25 00:00:00|\n",
      "|      16|2013-07-25 00:00:00|\n",
      "|      17|2013-07-25 00:00:00|\n",
      "|      18|2013-07-25 00:00:00|\n",
      "|      19|2013-07-25 00:00:00|\n",
      "|      20|2013-07-25 00:00:00|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "|      13|\n",
      "|      14|\n",
      "|      15|\n",
      "|      16|\n",
      "|      17|\n",
      "|      18|\n",
      "|      19|\n",
      "|      20|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDf = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\",True)\\\n",
    "         .option(\"inferSchema\",True)\\\n",
    "         .option(\"path\",\"orders-201019-002101.csv\")\\\n",
    "         .load()\n",
    "orderDf.select(\"order_id\",\"order_date\").show()\n",
    "orderDf.select(col(\"order_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52049205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our own user defined function is spark.\n",
    "#1. Column object expression -- the function won't be registered in catalog\n",
    "#2. SQL expression -- the function will be registered in catalog.So in this case we can even use it with spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a50b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the age is greater than 18 we have to populate the 4th column named \n",
    "#Adult with \"Y\"else we need to populated the column with \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e0186be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\",True)\\\n",
    "    .option(\"path\",\"-201025-223502.dataset1\")\\\n",
    "    .load()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd9cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    _c0|_c1|      _c2|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7c53306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- adult: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----+\n",
      "|   name|age|     city|adult|\n",
      "+-------+---+---------+-----+\n",
      "|  sumit| 30|bangalore|    Y|\n",
      "|  kapil| 32|hyderabad|    Y|\n",
      "|sathish| 16|  chennai|    N|\n",
      "|   ravi| 39|bangalore|    Y|\n",
      "| kavita| 12|hyderabad|    N|\n",
      "|  kavya| 19|   mysore|    Y|\n",
      "+-------+---+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.toDF(\"name\",\"age\",\"city\")\n",
    "def ageCheck(age):\n",
    "    if(age > 18):\n",
    "        return \"Y\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "parseAgeFunction = udf(ageCheck,StringType())\n",
    "df2 = df1.withColumn(\"adult\",parseAgeFunction(\"age\"))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6316fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 13:06:54 WARN SimpleFunctionRegistry: The function parseagefunction replaced a previously registered function.\n",
      "Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True)\n",
      "Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True)\n",
      "Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True)\n",
      "Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True)\n",
      "Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True)\n",
      "Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True)\n",
      "Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True)\n",
      "Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True)\n",
      "Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True)\n",
      "Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True)\n",
      "Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True)\n",
      "Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True)\n",
      "Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True)\n",
      "Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True)\n",
      "Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True)\n",
      "Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True)\n",
      "Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True)\n",
      "Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True)\n",
      "Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True)\n",
      "Function(name='aes_decrypt', description=None, className='org.apache.spark.sql.catalyst.expressions.AesDecrypt', isTemporary=True)\n",
      "Function(name='aes_encrypt', description=None, className='org.apache.spark.sql.catalyst.expressions.AesEncrypt', isTemporary=True)\n",
      "Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True)\n",
      "Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True)\n",
      "Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True)\n",
      "Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True)\n",
      "Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True)\n",
      "Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True)\n",
      "Function(name='array_agg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True)\n",
      "Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True)\n",
      "Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True)\n",
      "Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True)\n",
      "Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True)\n",
      "Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True)\n",
      "Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True)\n",
      "Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True)\n",
      "Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True)\n",
      "Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True)\n",
      "Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True)\n",
      "Function(name='array_size', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySize', isTemporary=True)\n",
      "Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True)\n",
      "Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True)\n",
      "Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True)\n",
      "Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True)\n",
      "Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True)\n",
      "Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True)\n",
      "Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True)\n",
      "Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True)\n",
      "Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True)\n",
      "Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True)\n",
      "Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True)\n",
      "Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True)\n",
      "Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True)\n",
      "Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True)\n",
      "Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True)\n",
      "Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True)\n",
      "Function(name='bit_get', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True)\n",
      "Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True)\n",
      "Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True)\n",
      "Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True)\n",
      "Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True)\n",
      "Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True)\n",
      "Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True)\n",
      "Function(name='btrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True)\n",
      "Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True)\n",
      "Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True)\n",
      "Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True)\n",
      "Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True)\n",
      "Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True)\n",
      "Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True)\n",
      "Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True)\n",
      "Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True)\n",
      "Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True)\n",
      "Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True)\n",
      "Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True)\n",
      "Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True)\n",
      "Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True)\n",
      "Function(name='contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ContainsExpressionBuilder', isTemporary=True)\n",
      "Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True)\n",
      "Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True)\n",
      "Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True)\n",
      "Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True)\n",
      "Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True)\n",
      "Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True)\n",
      "Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True)\n",
      "Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True)\n",
      "Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True)\n",
      "Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True)\n",
      "Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True)\n",
      "Function(name='csc', description=None, className='org.apache.spark.sql.catalyst.expressions.Csc', isTemporary=True)\n",
      "Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True)\n",
      "Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True)\n",
      "Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True)\n",
      "Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True)\n",
      "Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True)\n",
      "Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True)\n",
      "Function(name='current_user', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True)\n",
      "Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True)\n",
      "Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True)\n",
      "Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True)\n",
      "Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePartExpressionBuilder', isTemporary=True)\n",
      "Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True)\n",
      "Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True)\n",
      "Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True)\n",
      "Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True)\n",
      "Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True)\n",
      "Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True)\n",
      "Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True)\n",
      "Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True)\n",
      "Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True)\n",
      "Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True)\n",
      "Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True)\n",
      "Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True)\n",
      "Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True)\n",
      "Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True)\n",
      "Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True)\n",
      "Function(name='endswith', description=None, className='org.apache.spark.sql.catalyst.expressions.EndsWithExpressionBuilder', isTemporary=True)\n",
      "Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True)\n",
      "Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True)\n",
      "Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True)\n",
      "Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True)\n",
      "Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True)\n",
      "Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True)\n",
      "Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True)\n",
      "Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True)\n",
      "Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True)\n",
      "Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True)\n",
      "Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True)\n",
      "Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True)\n",
      "Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True)\n",
      "Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.FloorExpressionBuilder', isTemporary=True)\n",
      "Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True)\n",
      "Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True)\n",
      "Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True)\n",
      "Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True)\n",
      "Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True)\n",
      "Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True)\n",
      "Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True)\n",
      "Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True)\n",
      "Function(name='getbit', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True)\n",
      "Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True)\n",
      "Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True)\n",
      "Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True)\n",
      "Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True)\n",
      "Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True)\n",
      "Function(name='histogram_numeric', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HistogramNumeric', isTemporary=True)\n",
      "Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True)\n",
      "Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True)\n",
      "Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True)\n",
      "Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True)\n",
      "Function(name='ilike', description=None, className='org.apache.spark.sql.catalyst.expressions.ILike', isTemporary=True)\n",
      "Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True)\n",
      "Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True)\n",
      "Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True)\n",
      "Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True)\n",
      "Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True)\n",
      "Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True)\n",
      "Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True)\n",
      "Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True)\n",
      "Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True)\n",
      "Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True)\n",
      "Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True)\n",
      "Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True)\n",
      "Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True)\n",
      "Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True)\n",
      "Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True)\n",
      "Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True)\n",
      "Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True)\n",
      "Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True)\n",
      "Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True)\n",
      "Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True)\n",
      "Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True)\n",
      "Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True)\n",
      "Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True)\n",
      "Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True)\n",
      "Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True)\n",
      "Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True)\n",
      "Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True)\n",
      "Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True)\n",
      "Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True)\n",
      "Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True)\n",
      "Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True)\n",
      "Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True)\n",
      "Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True)\n",
      "Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True)\n",
      "Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.LPadExpressionBuilder', isTemporary=True)\n",
      "Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True)\n",
      "Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True)\n",
      "Function(name='make_dt_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True)\n",
      "Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True)\n",
      "Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True)\n",
      "Function(name='make_ym_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True)\n",
      "Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True)\n",
      "Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True)\n",
      "Function(name='map_contains_key', description=None, className='org.apache.spark.sql.catalyst.expressions.MapContainsKey', isTemporary=True)\n",
      "Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True)\n",
      "Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True)\n",
      "Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True)\n",
      "Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True)\n",
      "Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True)\n",
      "Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True)\n",
      "Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True)\n",
      "Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True)\n",
      "Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True)\n",
      "Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True)\n",
      "Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True)\n",
      "Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True)\n",
      "Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True)\n",
      "Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True)\n",
      "Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True)\n",
      "Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True)\n",
      "Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True)\n",
      "Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True)\n",
      "Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True)\n",
      "Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True)\n",
      "Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True)\n",
      "Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True)\n",
      "Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True)\n",
      "Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True)\n",
      "Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True)\n",
      "Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True)\n",
      "Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True)\n",
      "Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True)\n",
      "Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True)\n",
      "Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True)\n",
      "Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True)\n",
      "Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True)\n",
      "Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True)\n",
      "Function(name='parseAgeFunction', description=None, className=None, isTemporary=True)\n",
      "Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True)\n",
      "Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True)\n",
      "Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True)\n",
      "Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True)\n",
      "Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True)\n",
      "Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True)\n",
      "Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True)\n",
      "Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True)\n",
      "Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True)\n",
      "Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True)\n",
      "Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True)\n",
      "Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True)\n",
      "Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True)\n",
      "Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True)\n",
      "Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True)\n",
      "Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True)\n",
      "Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True)\n",
      "Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True)\n",
      "Function(name='range', description=None, className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True)\n",
      "Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True)\n",
      "Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True)\n",
      "Function(name='regexp', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True)\n",
      "Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True)\n",
      "Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True)\n",
      "Function(name='regexp_like', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True)\n",
      "Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True)\n",
      "Function(name='regr_avgx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgX', isTemporary=True)\n",
      "Function(name='regr_avgy', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgY', isTemporary=True)\n",
      "Function(name='regr_count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrCount', isTemporary=True)\n",
      "Function(name='regr_r2', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrR2', isTemporary=True)\n",
      "Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True)\n",
      "Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True)\n",
      "Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True)\n",
      "Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True)\n",
      "Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True)\n",
      "Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True)\n",
      "Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True)\n",
      "Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True)\n",
      "Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.RPadExpressionBuilder', isTemporary=True)\n",
      "Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True)\n",
      "Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True)\n",
      "Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True)\n",
      "Function(name='sec', description=None, className='org.apache.spark.sql.catalyst.expressions.Sec', isTemporary=True)\n",
      "Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True)\n",
      "Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True)\n",
      "Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True)\n",
      "Function(name='session_window', description=None, className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True)\n",
      "Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True)\n",
      "Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True)\n",
      "Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True)\n",
      "Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True)\n",
      "Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True)\n",
      "Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True)\n",
      "Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True)\n",
      "Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True)\n",
      "Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True)\n",
      "Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True)\n",
      "Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True)\n",
      "Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True)\n",
      "Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True)\n",
      "Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True)\n",
      "Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True)\n",
      "Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True)\n",
      "Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True)\n",
      "Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True)\n",
      "Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True)\n",
      "Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True)\n",
      "Function(name='split_part', description=None, className='org.apache.spark.sql.catalyst.expressions.SplitPart', isTemporary=True)\n",
      "Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True)\n",
      "Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True)\n",
      "Function(name='startswith', description=None, className='org.apache.spark.sql.catalyst.expressions.StartsWithExpressionBuilder', isTemporary=True)\n",
      "Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True)\n",
      "Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True)\n",
      "Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True)\n",
      "Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True)\n",
      "Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True)\n",
      "Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True)\n",
      "Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True)\n",
      "Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True)\n",
      "Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True)\n",
      "Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True)\n",
      "Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True)\n",
      "Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True)\n",
      "Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True)\n",
      "Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True)\n",
      "Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True)\n",
      "Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True)\n",
      "Function(name='to_binary', description=None, className='org.apache.spark.sql.catalyst.expressions.ToBinary', isTemporary=True)\n",
      "Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True)\n",
      "Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True)\n",
      "Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True)\n",
      "Function(name='to_number', description=None, className='org.apache.spark.sql.catalyst.expressions.ToNumber', isTemporary=True)\n",
      "Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True)\n",
      "Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True)\n",
      "Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True)\n",
      "Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True)\n",
      "Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True)\n",
      "Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True)\n",
      "Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True)\n",
      "Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True)\n",
      "Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True)\n",
      "Function(name='try_add', description=None, className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True)\n",
      "Function(name='try_avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.TryAverage', isTemporary=True)\n",
      "Function(name='try_divide', description=None, className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True)\n",
      "Function(name='try_element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.TryElementAt', isTemporary=True)\n",
      "Function(name='try_multiply', description=None, className='org.apache.spark.sql.catalyst.expressions.TryMultiply', isTemporary=True)\n",
      "Function(name='try_subtract', description=None, className='org.apache.spark.sql.catalyst.expressions.TrySubtract', isTemporary=True)\n",
      "Function(name='try_sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.TrySum', isTemporary=True)\n",
      "Function(name='try_to_binary', description=None, className='org.apache.spark.sql.catalyst.expressions.TryToBinary', isTemporary=True)\n",
      "Function(name='try_to_number', description=None, className='org.apache.spark.sql.catalyst.expressions.TryToNumber', isTemporary=True)\n",
      "Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True)\n",
      "Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True)\n",
      "Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True)\n",
      "Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True)\n",
      "Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True)\n",
      "Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True)\n",
      "Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True)\n",
      "Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True)\n",
      "Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True)\n",
      "Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True)\n",
      "Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True)\n",
      "Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True)\n",
      "Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True)\n",
      "Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True)\n",
      "Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True)\n",
      "Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True)\n",
      "Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True)\n",
      "Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True)\n",
      "Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True)\n",
      "Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True)\n",
      "Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True)\n",
      "Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True)\n",
      "Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True)\n",
      "Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True)\n",
      "Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True)\n",
      "Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True)\n",
      "Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True)\n",
      "Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True)\n",
      "Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True)\n",
      "Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True)\n",
      "Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True)\n",
      "Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True)\n",
      "Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True)\n",
      "Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"parseAgeFunction\",ageCheck,StringType())\n",
    "for x in spark.catalog.listFunctions():\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4872f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----+\n",
      "|   name|age|     city|adult|\n",
      "+-------+---+---------+-----+\n",
      "|  sumit| 30|bangalore|    Y|\n",
      "|  kapil| 32|hyderabad|    Y|\n",
      "|sathish| 16|  chennai|    N|\n",
      "|   ravi| 39|bangalore|    Y|\n",
      "| kavita| 12|hyderabad|    N|\n",
      "|  kavya| 19|   mysore|    Y|\n",
      "+-------+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"adult\",expr(\"parseAgeFunction(age)\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857e467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the spark session\n",
    "#create a local list\n",
    "#create a dataframe from this local list and give column names\n",
    "#add a new column date1 with unix timestamp\n",
    "#add one more column with monotonically increasing id\n",
    "\n",
    "#drop the duplicates based on combination of 2 columns\n",
    "#drop the orderid column\n",
    "#sort based on order date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba955ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [(1,\"2013-07-25\",11599,\"CLOSED\"),\n",
    "          (2,\"2014-07-25\",256,\"PENDING_PAYMENT\"),\n",
    "          (3,\"2013-07-25\",11599,\"COMPLETE\"),\n",
    "          (4,\"2019-07-25\",8827,\"CLOSED\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf0c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf = spark.createDataFrame(myList)\\\n",
    "                .toDF(\"orderid\",\"orderdate\",\"customerid\",\"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2c6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf = ordersDf\\\n",
    "       .withColumn(\"date1\",unix_timestamp(col(\"orderdate\"))) \\\n",
    "       .withColumn(\"newid\", monotonically_increasing_id()) \\\n",
    "       .dropDuplicates([\"orderdate\",\"customerid\"])\\\n",
    "       .drop(\"orderid\")\\\n",
    "       .sort(\"orderdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56007d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: long (nullable = true)\n",
      " |-- orderdate: string (nullable = true)\n",
      " |-- customerid: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------------+\n",
      "|orderid| orderdate|customerid|         status|\n",
      "+-------+----------+----------+---------------+\n",
      "|      1|2013-07-25|     11599|         CLOSED|\n",
      "|      2|2014-07-25|       256|PENDING_PAYMENT|\n",
      "|      3|2013-07-25|     11599|       COMPLETE|\n",
      "|      4|2019-07-25|      8827|         CLOSED|\n",
      "+-------+----------+----------+---------------+\n",
      "\n",
      "+----------+----------+---------------+-----+-----+\n",
      "| orderdate|customerid|         status|date1|newid|\n",
      "+----------+----------+---------------+-----+-----+\n",
      "|2013-07-25|     11599|         CLOSED| null|    0|\n",
      "|2014-07-25|       256|PENDING_PAYMENT| null|    1|\n",
      "|2019-07-25|      8827|         CLOSED| null|    3|\n",
      "+----------+----------+---------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.printSchema()\n",
    "ordersDf.show()\n",
    "newDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787cb2f",
   "metadata": {},
   "source": [
    "# Aggregate transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbeadc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Simple aggregations\n",
    "#2. Grouping aggregations\n",
    "#3. window aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f1844",
   "metadata": {},
   "source": [
    "# simple aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec2987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "invoiceDF = spark.read.format(\"csv\")\\\n",
    "           .option(\"header\",True)\\\n",
    "           .option(\"inferSchema\",True)\\\n",
    "           .option(\"path\",\"order_data-201025-223502.csv\")\\\n",
    "           .load()\n",
    "#column object expression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "342639ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+-------------+\n",
      "|RowCount|TotalQuantity|         AvgPrice|CountDistinct|\n",
      "+--------+-------------+-----------------+-------------+\n",
      "|  541782|      5175855|4.611565323321897|        25858|\n",
      "+--------+-------------+-----------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "invoiceDF.select(count(\"*\").alias(\"RowCount\"),\n",
    "                 sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "                 avg(\"UnitPrice\").alias(\"AvgPrice\"),\n",
    "                 countDistinct(\"InvoiceNo\").alias(\"CountDistinct\"))\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506cd3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+-------------+\n",
      "|RowCount|TotalQuantity|         AvgPrice|CountDistinct|\n",
      "+--------+-------------+-----------------+-------------+\n",
      "|  541782|      5175855|4.611565323321897|        25858|\n",
      "+--------+-------------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#column string expression\n",
    "invoiceDF.selectExpr(\"count(*) as RowCount\",\"sum(Quantity) as TotalQuantity\",\n",
    "                     \"avg(UnitPrice) as AvgPrice\",\n",
    "                     \"count(Distinct(InvoiceNo)) as CountDistinct\")\\\n",
    "                     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42b5e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+-------------------------+\n",
      "|count(1)|sum(Quantity)|   avg(UnitPrice)|count(DISTINCT InvoiceNo)|\n",
      "+--------+-------------+-----------------+-------------------------+\n",
      "|  541782|      5175855|4.611565323321897|                    25858|\n",
      "+--------+-------------+-----------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark SQL\n",
    "invoiceDF.createOrReplaceTempView(\"sales\")\n",
    "spark.sql(\"select count(*),sum(Quantity),avg(UnitPrice),count(distinct(InvoiceNo)) from sales\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca726d",
   "metadata": {},
   "source": [
    "# Grouping aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12b34ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+------------------+\n",
      "|       Country|InvoiceNo|TotalQuantity|      InvoiceValue|\n",
      "+--------------+---------+-------------+------------------+\n",
      "|United Kingdom|   536446|          329|            440.89|\n",
      "|United Kingdom|   536508|          216|            155.52|\n",
      "|United Kingdom|   537811|           74|            268.86|\n",
      "|United Kingdom|   538895|          370|            247.38|\n",
      "|United Kingdom|   540453|          341|302.44999999999993|\n",
      "|United Kingdom|   541291|          217|305.81000000000006|\n",
      "|United Kingdom|   542551|           -1|               0.0|\n",
      "|United Kingdom|   542576|           -1|               0.0|\n",
      "|United Kingdom|   542628|            9|            132.35|\n",
      "|United Kingdom|   542886|          199| 320.5099999999998|\n",
      "|United Kingdom|   542907|           75|            313.85|\n",
      "|United Kingdom|   543131|          134|             164.1|\n",
      "|United Kingdom|   543189|          102|            153.94|\n",
      "|United Kingdom|   543265|           -4|               0.0|\n",
      "|        Cyprus|   544574|          173|            320.69|\n",
      "|United Kingdom|   545077|           24|             10.08|\n",
      "|United Kingdom|   545300|          116|            323.16|\n",
      "|United Kingdom|   545347|           72| 76.32000000000001|\n",
      "|United Kingdom|   545418|           10|              85.0|\n",
      "|United Kingdom|   545897|          577|1762.2200000000018|\n",
      "+--------------+---------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#column object expression\n",
    "summaryDF = invoiceDF.groupBy(\"Country\",\"InvoiceNo\")\\\n",
    "           .agg(sum(\"Quantity\").alias(\"TotalQuantity\"),sum(expr(\"Quantity * UnitPrice\")).alias(\"InvoiceValue\"))\n",
    "summaryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e34aa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+------------------+\n",
      "|       Country|InvoiceNo|TotalQunatity|      InvoiceValue|\n",
      "+--------------+---------+-------------+------------------+\n",
      "|United Kingdom|   536446|          329|            440.89|\n",
      "|United Kingdom|   536508|          216|            155.52|\n",
      "|United Kingdom|   537811|           74|            268.86|\n",
      "|United Kingdom|   538895|          370|            247.38|\n",
      "|United Kingdom|   540453|          341|302.44999999999993|\n",
      "|United Kingdom|   541291|          217|305.81000000000006|\n",
      "|United Kingdom|   542551|           -1|               0.0|\n",
      "|United Kingdom|   542576|           -1|               0.0|\n",
      "|United Kingdom|   542628|            9|            132.35|\n",
      "|United Kingdom|   542886|          199| 320.5099999999998|\n",
      "|United Kingdom|   542907|           75|            313.85|\n",
      "|United Kingdom|   543131|          134|             164.1|\n",
      "|United Kingdom|   543189|          102|            153.94|\n",
      "|United Kingdom|   543265|           -4|               0.0|\n",
      "|        Cyprus|   544574|          173|            320.69|\n",
      "|United Kingdom|   545077|           24|             10.08|\n",
      "|United Kingdom|   545300|          116|            323.16|\n",
      "|United Kingdom|   545347|           72| 76.32000000000001|\n",
      "|United Kingdom|   545418|           10|              85.0|\n",
      "|United Kingdom|   545897|          577|1762.2200000000018|\n",
      "+--------------+---------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#string expression\n",
    "summaryDf1 = invoiceDF.groupBy(\"Country\",\"InvoiceNo\")\\\n",
    "            .agg(expr(\"sum(Quantity) as TotalQunatity\"),expr(\"sum(Quantity * UnitPrice) as InvoiceValue\"))\n",
    "summaryDf1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "378bf9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------+------------------+\n",
      "|       country|InvoiceNo|totQty|    asInvoiceValue|\n",
      "+--------------+---------+------+------------------+\n",
      "|United Kingdom|   536446|   329|            440.89|\n",
      "|United Kingdom|   536508|   216|            155.52|\n",
      "|United Kingdom|   537811|    74|            268.86|\n",
      "|United Kingdom|   538895|   370|            247.38|\n",
      "|United Kingdom|   540453|   341|302.44999999999993|\n",
      "|United Kingdom|   541291|   217|305.81000000000006|\n",
      "|United Kingdom|   542551|    -1|               0.0|\n",
      "|United Kingdom|   542576|    -1|               0.0|\n",
      "|United Kingdom|   542628|     9|            132.35|\n",
      "|United Kingdom|   542886|   199| 320.5099999999998|\n",
      "|United Kingdom|   542907|    75|            313.85|\n",
      "|United Kingdom|   543131|   134|             164.1|\n",
      "|United Kingdom|   543189|   102|            153.94|\n",
      "|United Kingdom|   543265|    -4|               0.0|\n",
      "|        Cyprus|   544574|   173|            320.69|\n",
      "|United Kingdom|   545077|    24|             10.08|\n",
      "|United Kingdom|   545300|   116|            323.16|\n",
      "|United Kingdom|   545347|    72| 76.32000000000001|\n",
      "|United Kingdom|   545418|    10|              85.0|\n",
      "|United Kingdom|   545897|   577|1762.2200000000018|\n",
      "+--------------+---------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#spark SQL\n",
    "invoiceDF.createOrReplaceTempView(\"sales\")\n",
    "spark.sql(\"\"\"select  country,InvoiceNo,sum(Quantity) as totQty,sum(Quantity * UnitPrice) asInvoiceValue \n",
    "         from sales group by country,InvoiceNo\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb530",
   "metadata": {},
   "source": [
    "# 3. window aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b57286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoiceDF = spark.read \\\n",
    "           .format(\"csv\") \\\n",
    "           .option(\"header\", True) \\\n",
    "           .option(\"inferSchema\", True) \\\n",
    "           .option(\"path\", \"windowdata-201025-223502.csv\") \\\n",
    "           .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "142b6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "myWindow = Window.partitionBy(\"country\")\\\n",
    "          .orderBy(\"weeknum\")\\\n",
    "          .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92798293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----------+-------------+------------+------------------+\n",
      "|        country|weeknum|numinvoices|totalquantity|invoicevalue|      RunningTotal|\n",
      "+---------------+-------+-----------+-------------+------------+------------------+\n",
      "|      Australia|     48|          1|          107|      358.25|            358.25|\n",
      "|      Australia|     49|          1|          214|       258.9|            617.15|\n",
      "|      Australia|     50|          2|          133|      387.95|1005.0999999999999|\n",
      "|        Austria|     50|          2|            3|      257.04|            257.04|\n",
      "|        Bahrain|     51|          1|           54|      205.74|            205.74|\n",
      "|        Belgium|     48|          1|          528|       346.1|             346.1|\n",
      "|        Belgium|     50|          2|          285|      625.16|            971.26|\n",
      "|        Belgium|     51|          2|          942|      838.65|1809.9099999999999|\n",
      "|Channel Islands|     49|          1|           80|      363.53|            363.53|\n",
      "|         Cyprus|     50|          1|          917|     1590.82|           1590.82|\n",
      "|        Denmark|     49|          1|          454|      1281.5|            1281.5|\n",
      "|        Finland|     50|          1|         1254|       892.8|             892.8|\n",
      "|         France|     48|          4|         1299|     2808.16|           2808.16|\n",
      "|         France|     49|          9|         2303|     4527.01|           7335.17|\n",
      "|         France|     50|          6|          529|      537.32|           7872.49|\n",
      "|         France|     51|          5|          847|     1702.87|           9575.36|\n",
      "|        Germany|     48|         11|         1795|     3309.75|           3309.75|\n",
      "|        Germany|     49|         12|         1852|     4521.39|           7831.14|\n",
      "|        Germany|     50|         15|         1973|     5065.79|          12896.93|\n",
      "|        Germany|     51|          5|         1103|     1665.91|          14562.84|\n",
      "+---------------+-------+-----------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydf = invoiceDF.withColumn(\"RunningTotal\",sum(\"invoicevalue\").over(myWindow))\n",
    "mydf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e86add",
   "metadata": {},
   "source": [
    "# join on data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8278529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two kind of join\n",
    "#1-simple join\n",
    "#2-broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdbe1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. inner (matching records from both the tables) we wont see the customer who never placed a order.\n",
    "#2. outer - matching records + non matching records from left table + non matching records from right table \n",
    "#3. left - matching records + non matching records from the left table\n",
    "#4. right - matching records + non matching records fromthe right table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c353b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = spark.read \\\n",
    "           .format(\"csv\") \\\n",
    "           .option(\"header\", True) \\\n",
    "           .option(\"inferSchema\", True) \\\n",
    "           .option(\"path\", \"orders-201019-002101.csv\") \\\n",
    "           .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bdb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF = spark.read \\\n",
    "           .format(\"csv\") \\\n",
    "           .option(\"header\", True) \\\n",
    "           .option(\"inferSchema\", True) \\\n",
    "           .option(\"path\", \"customers-201025-223502.csv\") \\\n",
    "           .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302cb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orderDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e231b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customerDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b94eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinCondition= orderDF.order_customer_id == customerDF.customer_id\n",
    "joinType=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e08651",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF= orderDF.join(customerDF,joinCondition,joinType).sort(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0debbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|   68884|               null|             null|       COMPLETE|       null|          null|          null|          null|             null|                null|         null|          null|            null|\n",
      "|   22945|2013-12-13 00:00:00|                1|       COMPLETE|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|   15192|2013-10-29 00:00:00|                2|PENDING_PAYMENT|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|   67863|2013-11-30 00:00:00|                2|       COMPLETE|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|   33865|2014-02-18 00:00:00|                2|       COMPLETE|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|   57963|2013-08-02 00:00:00|                2|        ON_HOLD|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|   61453|2013-12-14 00:00:00|                3|       COMPLETE|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   22646|2013-12-11 00:00:00|                3|       COMPLETE|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   23662|2013-12-19 00:00:00|                3|       COMPLETE|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   56178|2014-07-15 00:00:00|                3|        PENDING|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   57617|2014-07-24 00:00:00|                3|       COMPLETE|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   35158|2014-02-26 00:00:00|                3|       COMPLETE|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|   46399|2014-05-09 00:00:00|                3|     PROCESSING|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|    9704|2013-09-24 00:00:00|                4|       COMPLETE|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|   37878|2014-03-15 00:00:00|                4|       COMPLETE|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|    9023|2013-09-19 00:00:00|                4|       COMPLETE|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|   49339|2014-05-28 00:00:00|                4|       COMPLETE|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|   17253|2013-11-09 00:00:00|                4|PENDING_PAYMENT|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|   51157|2014-06-10 00:00:00|                4|         CLOSED|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|   13705|2013-10-18 00:00:00|                5|       COMPLETE|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|             725|\n",
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3840c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in joinDF:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e2a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=joinDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49b5a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68883\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in k:\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb481a",
   "metadata": {},
   "source": [
    "# Problem after joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04c780c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. showcasing how your code can lead to ambiguouscolumn names.\n",
    "#this happens when we try to select a column name which is coming from 2 different dataframes..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e42e75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am going to rename the column name in order.csv as customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ba5ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = spark.read \\\n",
    "           .format(\"csv\") \\\n",
    "           .option(\"header\", True) \\\n",
    "           .option(\"inferSchema\", True) \\\n",
    "           .option(\"path\", \"orders-201019-002101.csv\") \\\n",
    "           .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deefc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF = spark.read \\\n",
    "           .format(\"csv\") \\\n",
    "           .option(\"header\", True) \\\n",
    "           .option(\"inferSchema\", True) \\\n",
    "           .option(\"path\", \"customers-201025-223502.csv\") \\\n",
    "           .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb39c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinCondition= orderDF.customer_id == customerDF.customer_id\n",
    "joinType=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d066d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|order_id|         order_date|customer_id|   order_status|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|      12111|         Amber|        Franco|     XXXXXXXXX|        XXXXXXXXX|8766 Clear Prairi...|   Santa Cruz|            CA|           95060|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|       8827|         Brian|        Wilson|     XXXXXXXXX|        XXXXXXXXX|   8396 High Corners|  San Antonio|            TX|           78240|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|      11318|          Mary|         Henry|     XXXXXXXXX|        XXXXXXXXX|3047 Silent Ember...|       Caguas|            PR|             725|\n",
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.join(customerDF,joinCondition,joinType).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdded815",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'customer_id' is ambiguous, could be: customer_id, customer_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7r/x4_8ch8j5w547gls5xdcg2_40000gn/T/ipykernel_81272/643114456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#here we are getting customer_id twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0morderDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomerDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoinCondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoinType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"order_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"order_status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2021\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \"\"\"\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Reference 'customer_id' is ambiguous, could be: customer_id, customer_id."
     ]
    }
   ],
   "source": [
    "#here we are getting customer_id twice\n",
    "\n",
    "orderDF.join(customerDF,joinCondition,joinType).select(\"customer_id\",\"order_id\",\"order_status\").show(5)\n",
    "# we will get an error because of ambiguous column in two different dataframe and system get confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e30950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to solve this issue.. ========================== \n",
    "#there are 2 ways to solve this problem \n",
    "#1. this is before the join\n",
    "#you rename the ambiguous column in one of the dataframe .withColumnRenamed(\"old_column_name\",\"new_column_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "208d7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF1=orderDF.withColumnRenamed(\"customer_id\",\"cust_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fc67702",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinCondition= orderDF1.cust_id == customerDF.customer_id\n",
    "joinType=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0c3cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|order_id|         order_date|cust_id|   order_status|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+--------+-------------------+-------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|\n",
      "+--------+-------------------+-------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF1.join(customerDF,joinCondition,joinType).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9708de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------------+\n",
      "|customer_id|order_id|   order_status|\n",
      "+-----------+--------+---------------+\n",
      "|      11599|       1|         CLOSED|\n",
      "|        256|       2|PENDING_PAYMENT|\n",
      "+-----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF1.join(customerDF,joinCondition,joinType).select(\"customer_id\",\"order_id\",\"order_status\").show(2)\n",
    "#NOW WE WILL GET THE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7e07453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. once the join is done we can drop one of those columns. \n",
    "#.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51dfdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinCondition= orderDF.customer_id == customerDF.customer_id\n",
    "joinType=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "917f0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=orderDF.join(customerDF,joinCondition,joinType).drop(orderDF.customer_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b85849bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------------+\n",
      "|customer_id|order_id|   order_status|\n",
      "+-----------+--------+---------------+\n",
      "|      11599|       1|         CLOSED|\n",
      "|        256|       2|PENDING_PAYMENT|\n",
      "|      12111|       3|       COMPLETE|\n",
      "|       8827|       4|         CLOSED|\n",
      "|      11318|       5|       COMPLETE|\n",
      "+-----------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.select(\"customer_id\",\"order_id\",\"order_status\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e3cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
